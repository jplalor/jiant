// config for artificial crowd generation, so that we can get a variety of response
// patterns for irt model fitting
//
// you will have to pass in the training data fraction 
// Run with:
//   python main.py --config_file config/artificial_crowd.conf --overrides "training_data_fraction $1 run_name ac-run-$1"
//

// This file uses HOCON, which is a JSON/YAML-like format that supports
// includes, references, and object merging semantics; see
// https://github.com/lightbend/config/blob/master/HOCON.md for reference.

// This imports the defaults, which can be overridden below.
include "defaults.conf"  // relative path to this file


exp_name = "artificial-crowd-generation-2"

pretrain_tasks = glue 
target_tasks = glue  


// Data and preprocessing settings
max_seq_len = 256 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.

// Model settings
input_module = "bert-base-uncased"
pytorch_transformers_output_mode = "top"
pair_attn = 0 // shouldnt be needed but JIC
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune // finetune entire BERT model

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 4
max_epochs = 10
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

// Control-flow stuff
do_pretrain = 1
do_target_task_training = 0
do_full_eval = 1
write_preds = "train,val,test"
write_strict_glue_format = 0

